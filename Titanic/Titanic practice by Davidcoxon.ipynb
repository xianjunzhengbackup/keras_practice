{
  "cells": [
    {
      "metadata": {
        "_cell_guid": "0f2e1b48-de36-4da8-9df9-a2909a92b461",
        "_uuid": "721884fd77000b8b4e69c2af9958f4412afd2cff"
      },
      "cell_type": "markdown",
      "source": "# Predicting Titanic Survivers\nLike Titanic, this is my maiden voyage,  when it comes to Kaggle contest that is!. I've completed the Data Science track on Data Camp, but I'm a relative newbie when it comes to machine learning. I'm going to attempt to work my way through the Titanic: Machine Learning contest. My aim is to submission and initial entry as quickly as possible to get a base line score and then attempt to improve on  on it by first looking at missing data, then engineering key features before establishing a  secondary base line and trying to improve the model itself. I'd like to be able to achieve a score of .80\n\nPlease feel free to post comments or  make suggestions as to what i may be doing wrong or could maybe do better and  consider upvoting if you find the notebook useful!\n\nBecause this notebook has built up over time I have commented out some of the lines that output files so that when i want to output and test a slight change to the code, i don't output files for bit of the notebook that haven't been changed and that i am not especially intereted in. If you are forking this code you can simple remove the hash and output the file. I have also experimented with different models, so the current model in any stage is not necessarily the most efficent (its just the one that i tried last)."
    },
    {
      "metadata": {
        "_cell_guid": "bb27af35-206d-4da2-8f33-63a6b6671c72",
        "_uuid": "13a824268233d8a6a7002be362847e2446aa2da6"
      },
      "cell_type": "markdown",
      "source": "# Import the Libraries and Data"
    },
    {
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_kg_hide-input": false,
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport os\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.svm import SVC\n\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.cross_validation import KFold\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.\n\ndf_train=pd.read_csv('../input/train.csv',sep=',')\ndf_test=pd.read_csv('../input/test.csv',sep=',')\ndf_data = df_train.append(df_test) # The entire data: train + test.\n\nPassengerId = df_test['PassengerId']\nSubmission=pd.DataFrame()\nSubmission['PassengerId'] = df_test['PassengerId']",
      "execution_count": 1,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "eb5476cd-91d9-483c-a3b7-1590073da4d8",
        "_uuid": "1ce9a1b19b5440668849af3010cefd2c84f1b4f6"
      },
      "cell_type": "markdown",
      "source": "# Stage 1 : Explore the Data and create a basic model on raw data"
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "# Explore the data Statistically"
    },
    {
      "metadata": {
        "_cell_guid": "f191bda6-f63b-420a-8ae0-f2e22607b425",
        "_uuid": "bb5839d5b2d1036f98090fb2ec950f03304caec9"
      },
      "cell_type": "markdown",
      "source": "### Number of rows and columns "
    },
    {
      "metadata": {
        "_cell_guid": "073e7a38-5e62-427e-ba8d-72a7a5334442",
        "_uuid": "47cafc06d2881a01e80469d40ed38cbea126652c",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# How big are the training and test datasets\nprint(df_train.shape)\nprint(\"----------------------------\")\nprint(df_test.shape)",
      "execution_count": 2,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "7bab27ce-09d8-43aa-8842-ee000e2d1e8d",
        "_uuid": "7e85c3850f8dd12a0e782456fb1c1986fbaf84b1"
      },
      "cell_type": "markdown",
      "source": "### Column Names"
    },
    {
      "metadata": {
        "_cell_guid": "4be833c5-e99c-4519-9b29-4d27699cfe12",
        "_uuid": "ec6fdc9350bc1e3ce83114c370008528e8109afa",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# What are the column names \ndf_train.columns",
      "execution_count": 3,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "a340d978-40e3-407d-aadd-78a10c765178",
        "_uuid": "1c3996b06fe7470613faf06c5f0341103ec349e8"
      },
      "cell_type": "markdown",
      "source": "### Data Types"
    },
    {
      "metadata": {
        "_cell_guid": "1e27d5dd-1d67-438f-87c3-ea4577f297cd",
        "_uuid": "4534fe968fece75089f136f647d5ee9f1408b21e",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# What type of data object are in each column and how many missing values are there\ndf_data.info()",
      "execution_count": 5,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "41a8e34c-ee85-4cdb-86d6-5f01e964c409",
        "_uuid": "f6b8fbba4bb96b7e3aeaa562176e60051846abc1"
      },
      "cell_type": "markdown",
      "source": "### Missing Data\n\nHow much Data is missing from the training and test datasets, how important is that data and how much data cleaning might be required."
    },
    {
      "metadata": {
        "_cell_guid": "f9bee28e-9b27-4f2b-8bf5-8e7a2b09ebbf",
        "_uuid": "77c81d38447b990a95f21afe3fae8bb5f6d00b4e",
        "scrolled": true,
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#check for any other unusable values\nprint(pd.isnull(df_data).sum())",
      "execution_count": 6,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "11c1e7bb-6f12-48ca-8807-add46fb447f4",
        "_uuid": "5bf733a2fc59ab5fd469aa07d107c06ffddc50a2"
      },
      "cell_type": "markdown",
      "source": "## Observations on missing data.\n\nThere are 144 missing ages in the training data and 86 mssing ages in the test data. Age is an important feature so it is worth spending time to address this properly. \n\nThere are 468 missing Cabin entries in the training data and 326 in the test data, at this stage I'm not sure how important this feature is so I'm going to revisit this when I know more about the feature.\nThere are 2 missing embarked data points in the train data and 1 missing fare in the test data, at this stage this does not represent a problem."
    },
    {
      "metadata": {
        "_cell_guid": "8807eb08-dd68-45b9-ab5e-d810a98a8cbe",
        "_uuid": "9d051a3476f6271b33a955216e947b7aea3de0f3"
      },
      "cell_type": "markdown",
      "source": "## Statistical Overview of the data"
    },
    {
      "metadata": {
        "_cell_guid": "817d81d9-6912-4771-b654-b3d5af85ed46",
        "_uuid": "10dbf2b79a79bbdcb283fb8237cf9a7eb94bccb2",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Get a statistical overview of the training data\ndf_train.describe()",
      "execution_count": 7,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "e1a5061c-c823-4768-9fde-1eec950c9a75",
        "_uuid": "0e37aa2ae972acdbaeb6366bb55c838eef8bc76e",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Get a statistical overview of the data\ndf_test.describe()",
      "execution_count": 10,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "53ce0b2e-724d-4530-87e4-9812597c1583",
        "_uuid": "7a99c489c6f184666d3e5586db6de3a441c1d0c6"
      },
      "cell_type": "markdown",
      "source": "Note: The mean and Std of each of the columns in the 2 datasets are reasonable close together, so its safe to assume that any relationships we discover in the  training data should work similarly in the test data."
    },
    {
      "metadata": {
        "_cell_guid": "37d3c26f-7b3e-411d-b939-ad874ba31eea",
        "_uuid": "ec577a60cb08406719037e966a974f29573b57fb",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Take a look at some sample data\ndf_train.head(5)",
      "execution_count": 9,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "9dd2bf45-bda2-4dab-bc69-8a8e46c1b72b",
        "_uuid": "30ca0953dbf13567cfccfd96bacf7e76f4474984",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "df_train.tail(5)",
      "execution_count": 8,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "2275818c-6652-4070-be61-b17a7e4b0b5e",
        "_uuid": "76fbda700a5f261b31759ade3e8c2c93d7458c01"
      },
      "cell_type": "markdown",
      "source": "# Explore Data Graphically"
    },
    {
      "metadata": {
        "_cell_guid": "76f9e3e9-db43-4cc4-a4cd-5f54003fe573",
        "_uuid": "45b16cde292ad9c45d0143e99a2be8f11ed9608b"
      },
      "cell_type": "markdown",
      "source": "## Survival by Age, Class and Gender"
    },
    {
      "metadata": {
        "_cell_guid": "158f3ef3-80fd-464d-a837-6b916926bbb2",
        "_uuid": "88e91d960cd9f7ec2fa3a635d93e69e54c4e338a",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "grid = sns.FacetGrid(df_train, col = \"Pclass\", row = \"Sex\", hue = \"Survived\", palette = 'seismic')\ngrid = grid.map(plt.scatter, \"PassengerId\", \"Age\")\ngrid.add_legend()\ngrid",
      "execution_count": 11,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "3cf55d17-8577-47cc-b81e-8c8fc89d60dc",
        "_uuid": "3d94018b485b98ae9744f17e6ead6f604061cbb6"
      },
      "cell_type": "markdown",
      "source": "## Survival by Age, Port of Embarkation and Gender"
    },
    {
      "metadata": {
        "_cell_guid": "e5f3871e-6638-426d-b0d9-5ed92d0ff1b5",
        "_uuid": "706c28dd41dc6ace4ba207bf027f59181cd14f05",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "grid = sns.FacetGrid(df_train, col = \"Embarked\", row = \"Sex\", hue = \"Survived\", palette = 'seismic')\ngrid = grid.map(plt.scatter, \"PassengerId\", \"Age\")\ngrid.add_legend()\ngrid",
      "execution_count": 12,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "be3ae2ae-f77b-4ae4-b8f2-09c5e6138907",
        "_uuid": "1687792f3f9915fe72cd2766681000ae74554361"
      },
      "cell_type": "markdown",
      "source": "This embarkation visualization indicates that a large proportion of passengers embarked at port 'S', with lesser numbers at 'C' and 'Q' it also shows that regardless of embarkation port more women survived than men. It doesn't seem to show any corelation between passenger ID and Embarkation port. Interestingly Embarkation port Q seems to indicate that  only 1 man survived while all women with passenger ID below 500 seem to survive while those above didn't this may be chance but it does look odd compared to 'S' and 'C'."
    },
    {
      "metadata": {
        "_cell_guid": "0d5d257f-0b2c-4f43-9090-c74053973f07",
        "_uuid": "c963f65c9a8f91b04102ce3e4d63ed1062f7d7ee"
      },
      "cell_type": "markdown",
      "source": "## Survival by Age, Number of Siblings and Gender"
    },
    {
      "metadata": {
        "_cell_guid": "82442bd9-cdfc-4f9b-9ebc-9f69238159aa",
        "_uuid": "b365e7b30eb770a2d1e77e4a95d9b4c1ced60c6b",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "grid = sns.FacetGrid(df_train, col = \"SibSp\", row = \"Sex\", hue = \"Survived\", palette = 'seismic')\ngrid = grid.map(plt.scatter, \"PassengerId\", \"Age\")\ngrid.add_legend()\ngrid",
      "execution_count": 13,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "10118d5d-4913-4804-93f8-f2d19954c8cd",
        "_uuid": "e7bcda55320028f7e6f8da737e88bba582f560c8"
      },
      "cell_type": "markdown",
      "source": "## Survival by Age, Number of parch and Gender"
    },
    {
      "metadata": {
        "_cell_guid": "e7e9b15d-5ace-4571-8462-eecb93f7d6ec",
        "_uuid": "8c5c3db479f52e13aa80b6025c351eda045e97fb",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "grid = sns.FacetGrid(df_train, col = \"Parch\", row = \"Sex\", hue = \"Survived\", palette = 'seismic')\ngrid = grid.map(plt.scatter, \"PassengerId\", \"Age\")\ngrid.add_legend()\ngrid",
      "execution_count": 14,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "f001f636-fc2a-4881-a81e-53edabf53242",
        "_uuid": "fb00c6099b7f836f3610e911075f78af6ca724f2"
      },
      "cell_type": "markdown",
      "source": "# Pairplots\n\nTo get a very basic idea of the relationships between the different features we can use pairplots from seaborn."
    },
    {
      "metadata": {
        "_cell_guid": "60f3f81f-4401-4659-b457-4200cabdf2b5",
        "_uuid": "b81a66434766fce1abfc6ad160ec6a62bee97887",
        "scrolled": false,
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "g = sns.pairplot(df_train[[u'Survived', u'Pclass', u'Sex', u'Age', u'Parch', u'Fare', u'Embarked']], hue='Survived', palette = 'seismic',size=4,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=50) )\ng.set(xticklabels=[])",
      "execution_count": 15,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "1de34739-550f-4384-a3e0-0ea807bb297e",
        "_uuid": "ab4504d0c13948980ed23f80d155fd7d31cd4301"
      },
      "cell_type": "markdown",
      "source": "# Create simple model\n\nCreate a baseline score by using old the standard numeric data on on a very basic model, this will be used to see how much any changes we make to the data or model improve performance."
    },
    {
      "metadata": {
        "_cell_guid": "dd1b861c-f4d7-4d2f-a131-9b6f59b48624",
        "_uuid": "ca99023e73866ccccf2b58dadd69242defaea90c",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "NUMERIC_COLUMNS=['Pclass','Age','SibSp','Parch','Fare']\n\n# create test and training data\ndata_to_train = df_train[NUMERIC_COLUMNS].fillna(-1000)\ny=df_train['Survived']\nX=data_to_train\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=21, stratify=y)\n\nfrom sklearn.svm import LinearSVC\n\nclf = SVC()\nclf.fit(X_train, y_train)\nlinear_svc = LinearSVC()\n\n# Print the accuracy\nprint(\"Accuracy: {}\".format(clf.score(X_test, y_test)))",
      "execution_count": 16,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "23cb1c05-31ae-4676-934e-62c0e3f53821",
        "_uuid": "a9a81d694ac9b10f1558318ca64ec38d689411e0"
      },
      "cell_type": "markdown",
      "source": "# Create initial predictions¶"
    },
    {
      "metadata": {
        "_cell_guid": "edb5f39c-2c88-4c42-9bde-f6375503cef3",
        "_uuid": "50e1643054e16b31c2234ba18df0176ae4c44696",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "test = df_test[NUMERIC_COLUMNS].fillna(-1000)\nSubmission['Survived']=clf.predict(test)\nprint(Submission.head())\nprint('predictions generated')\n",
      "execution_count": 17,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "262a3df0-1a62-40bb-be9c-48bbc2787847",
        "_uuid": "df612c7f8c676da096cf50c3640b882a9443d330"
      },
      "cell_type": "markdown",
      "source": "# Make first Submission"
    },
    {
      "metadata": {
        "_cell_guid": "3ab17c71-9324-48ee-815f-e3a92f9cd074",
        "_kg_hide-output": true,
        "_uuid": "5b2bf28e6ab4b43d8368df9606f2d3d18d1bd4ab",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# write data frame to csv file\n#Submission.set_index('PassengerId', inplace=True)\n#Submission.to_csv('myfirstsubmission.csv',sep=',')\nprint('file created')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "f66f0981-d9dd-4c9f-ac7a-f63fddb16dd4",
        "_uuid": "b50c6f50f7d672349871e631132d9f941ba1a02f"
      },
      "cell_type": "markdown",
      "source": "The result of this first submission was a score of 0.57894. This constitutes performing just above random, if i'd simply flipped a coin fair coin for each passenger i could have achieved this kind of score. So there is plenty of room for improvement."
    },
    {
      "metadata": {
        "_cell_guid": "3a0ea6c3-2148-408e-b1eb-12b9b4f5cf6b",
        "_uuid": "c4c7faeb87fbe3324c324a01a0dc9bac3e31829d"
      },
      "cell_type": "markdown",
      "source": "# Stage 2 : Clean Data & Engineer features to improve results\n\n \n\n## Cleaning the data : Filling in the blanks\nThere are a number of missing values, including fare, embarked, age and cabin. I started off simply using the average value for fare, embarked and age. However after doing some visual data analysis it became obvious that I could use other factors like title to make better estimates on age by simply using the average for people with the same title, the same applied to embarked where average based on fare would give a better estimate and fare based on embarked. \n\nCabin has so much missing data that it is likely that estimating cabin may add a level of noise to the data that would not be helpful.\n\n## Feature Engineering\nHere we are going to manipulate the existing data and try and create new features that our model can use. In some cases this is as simple as changing male and female to numeric data like 0 or 1. We can also separate categorical data to values between 1 and 5 for different titles, alternately we could put each categorical value into its own column, and mark columns with a 0 if they don't apply or a 1 if they do. So features may be quite complex so for example calculating cabin numbers based on surname, and ticket details. "
    },
    {
      "metadata": {
        "_cell_guid": "8cf4c617-baf1-4172-85e0-7ae1754e23a8",
        "_uuid": "fad3c30cc68e471b84be592166cc2a661c4a357f"
      },
      "cell_type": "markdown",
      "source": "## Estimate missing Fare Data based on Embarkation\nWhile there is relatively little missing Fare data, the range of possible values is large, so rather than using simply the media of all fares, we can look at the passenger class or embarkation port in order to use a more appropriate average. We'll start by looking at boxplots for the fares to ensure we are making soon assumptions before we go onto estimating the missing values."
    },
    {
      "metadata": {
        "_cell_guid": "dc5090b5-1bb7-4ede-805e-62d9d4901867",
        "_uuid": "7bec995c8fc6a1bdc6634d9eaeafd5f7acd617c4",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "fig, (ax1, ax2) = plt.subplots(ncols=2, sharey=True,figsize=(12,6))\nsns.boxplot(data = df_train, x = \"Pclass\", y = \"Fare\",ax=ax1);\nplt.figure(1)\nsns.boxplot(data = df_train, x = \"Embarked\", y = \"Fare\",ax=ax2);\nplt.show()",
      "execution_count": 18,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "2a05e7be-844e-4c77-96a2-bfb146afc694",
        "_uuid": "af9325ff33b555d3f2d69495baec14ad59dd19a0",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Fill the na values in Fare based on embarked data\nembarked = ['S', 'C', 'Q']\nfor port in embarked:\n    fare_to_impute = df_data.groupby('Embarked')['Fare'].median()[embarked.index(port)]\n    df_data.loc[(df_data['Fare'].isnull()) & (df_data['Embarked'] == port), 'Fare'] = fare_to_impute\n# Fare in df_train and df_test:\ndf_train[\"Fare\"] = df_data['Fare'][:891]\ndf_test[\"Fare\"] = df_data['Fare'][891:]\nprint('Missing Fares Estimated')",
      "execution_count": 19,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "58556287-b706-46e8-9e8f-e9289cb0f62d",
        "_uuid": "085e8bc212ff7478117381b570b32a048d69927b"
      },
      "cell_type": "markdown",
      "source": "## FareBand feature"
    },
    {
      "metadata": {
        "_cell_guid": "f7b6005b-a173-421c-ac94-ac2f14c9c183",
        "_uuid": "97b3eec73c454f13d81e0ae1c641fcb2e29f2318",
        "scrolled": true,
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#fill in missing Fare value in training set based on mean fare for that Pclass \nfor x in range(len(df_train[\"Fare\"])):\n    if pd.isnull(df_train[\"Fare\"][x]):\n        pclass = df_train[\"Pclass\"][x] #Pclass = 3\n        df_train[\"Fare\"][x] = round(df_train[df_train[\"Pclass\"] == pclass][\"Fare\"].mean(), 8)\n        \n#fill in missing Fare value in test set based on mean fare for that Pclass         \nfor x in range(len(df_test[\"Fare\"])):\n    if pd.isnull(df_test[\"Fare\"][x]):\n        pclass = df_test[\"Pclass\"][x] #Pclass = 3\n        df_test[\"Fare\"][x] = round(df_test[df_test[\"Pclass\"] == pclass][\"Fare\"].mean(), 8)\n        \n#map Fare values into groups of numerical values\ndf_data[\"FareBand\"] = pd.qcut(df_data['Fare'], 8, labels = [1, 2, 3, 4,5,6,7,8]).astype('int')\ndf_train[\"FareBand\"] = pd.qcut(df_train['Fare'], 8, labels = [1, 2, 3, 4,5,6,7,8]).astype('int')\ndf_test[\"FareBand\"] = pd.qcut(df_test['Fare'], 8, labels = [1, 2, 3, 4,5,6,7,8]).astype('int')\ndf_train[[\"FareBand\", \"Survived\"]].groupby([\"FareBand\"], as_index=False).mean()\nprint('FareBand feature created')",
      "execution_count": 20,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "5d9a0816-00c1-459a-92b1-019d91dfdda6",
        "_uuid": "612dd9c3bb41a17fc61cd83c7f887119389b6d72"
      },
      "cell_type": "markdown",
      "source": "*** Note:*** There are several ways that machine learning can evaluate data, you can use discrete data like fare, or you can make that data categorical by grouping it into bands as i have done here or your can take those categories and turn each category into a column. Different models work, differently depending on how you give them the data. I'm going to create all 3 different structures for some features like fare and age and see how they compare. You shoud not over emphasis a feature by using multiple structures of the same data in a model, we'll therefore filter the differnet stuctures before we evaluate the models. "
    },
    {
      "metadata": {
        "_cell_guid": "b872a2c8-166d-4e3c-842d-cb05d0dbe026",
        "_uuid": "667caecad9a8266c354211a559f109cc8045eab9"
      },
      "cell_type": "markdown",
      "source": "## Embarked Feature"
    },
    {
      "metadata": {
        "_cell_guid": "6562ae96-2a2c-475c-910b-26ee2aacb98e",
        "_uuid": "88e9ea060f910b0e6372e50c4e19adb406da6275",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#map each Embarked value to a numerical value\nembarked_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\ndf_data[\"Embarked\"] = df_data[\"Embarked\"].map(embarked_mapping)\n# split Embanked into df_train and df_test:\ndf_train[\"Embarked\"] = df_data[\"Embarked\"][:891]\ndf_test[\"Embarked\"] = df_data[\"Embarked\"][891:]\nprint('Embarked feature created')\ndf_data[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean()",
      "execution_count": 21,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "4820654b-3839-4a3b-9795-7edf8b8493ee",
        "_uuid": "03b822c2b89b94e3ce45d4882c38956a99144696"
      },
      "cell_type": "markdown",
      "source": "## Estimate missing Embarkation Data"
    },
    {
      "metadata": {
        "_cell_guid": "f0e3a8b6-d72f-41fd-81c7-7cbd71a859cf",
        "_uuid": "fa51fe14a830486e1ed5c23e4fe320f1a3392172",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Fill the na values in Embanked based on fareband data\nfareband = [1,2,3,4]\nfor fare in fareband:\n    embark_to_impute = df_data.groupby('FareBand')['Embarked'].median()[fare]\n    df_data.loc[(df_data['Embarked'].isnull()) & (df_data['FareBand'] == fare), 'Embarked'] = embark_to_impute\n# Fare in df_train and df_test:\ndf_train[\"Embarked\"] = df_data['Embarked'][:891]\ndf_test[\"Embarked\"] = df_data['Embarked'][891:]\nprint('Missing Embarkation Estimated')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "e8e87c51-45c4-461e-9377-d671baa59cb6",
        "_uuid": "08481cfb45e394cfdd55571b7f4848270037e75e"
      },
      "cell_type": "markdown",
      "source": "We will come back to fill in the missing age data a little later. Initially i created an estimate based on the mean age and standard deviation, using random numbers to evenly distribute age estimates, which worked, but actually there is a better way using title. As we have not yet extracted the title data yet, we will wait to estimate ages until we have. "
    },
    {
      "metadata": {
        "_cell_guid": "42133e77-1cd1-416d-9100-7479a80b1d89",
        "_uuid": "873300197bc688e1350aaf8fe230ad8c57d1c222"
      },
      "cell_type": "markdown",
      "source": "##  Gender Feature"
    },
    {
      "metadata": {
        "_cell_guid": "23e16cb2-8bf9-4904-b9db-9304368cc857",
        "_uuid": "d1b98799dea36fd35ebd29efdb9cf71b72aa0fae",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# convert categories to Columns\ndummies=pd.get_dummies(df_train[['Sex']], prefix_sep='_') #Gender\ndf_train = pd.concat([df_train, dummies], axis=1) \ntestdummies=pd.get_dummies(df_test[['Sex']], prefix_sep='_') #Gender\ndf_test = pd.concat([df_test, testdummies], axis=1) \nprint('Gender Feature added ')",
      "execution_count": 22,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "b4a970b2-3eda-466f-bf17-64998499ef38",
        "_uuid": "922c7dad035637b126e82c3d2f929cef696a642d",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#map each Gendre value to a numerical value\ngender_mapping = {\"female\": 0, \"male\": 1}\ndf_data[\"Sex\"] = df_data['Sex'].map(gender_mapping)\ndf_data[\"Sex\"]=df_data[\"Sex\"].astype('int')\n\n# Family_Survival in TRAIN_DF and TEST_DF:\ndf_train[\"Sex\"] = df_data[\"Sex\"][:891]\ndf_test[\"Sex\"] = df_data[\"Sex\"][891:]\nprint('Gender Category created')",
      "execution_count": 23,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "73a1e1f3-0c5e-42cc-8cf8-00cecd7d3721",
        "_uuid": "ba21b6f61bafeb38d67a873ac096c91b734d39b5"
      },
      "cell_type": "markdown",
      "source": "## Title Feature"
    },
    {
      "metadata": {
        "_cell_guid": "a87030f8-df3f-4e9d-ad2a-3605d3b20b0b",
        "_uuid": "37e03baec88e410f0b920fe9e9c77b9ccfdba5d9",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#Get titles\ndf_data[\"Title\"] = df_data.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\n#Unify common titles. \ndf_data[\"Title\"] = df_data[\"Title\"].replace('Mlle', 'Miss')\ndf_data[\"Title\"] = df_data[\"Title\"].replace('Master', 'Master')\ndf_data[\"Title\"] = df_data[\"Title\"].replace(['Mme', 'Dona', 'Ms'], 'Mrs')\ndf_data[\"Title\"] = df_data[\"Title\"].replace(['Jonkheer','Don'],'Mr')\ndf_data[\"Title\"] = df_data[\"Title\"].replace(['Capt','Major', 'Col','Rev','Dr'], 'Millitary')\ndf_data[\"Title\"] = df_data[\"Title\"].replace(['Lady', 'Countess','Sir'], 'Honor')\n\n# Age in df_train and df_test:\ndf_train[\"Title\"] = df_data['Title'][:891]\ndf_test[\"Title\"] = df_data['Title'][891:]\n\n# convert Title categories to Columns\ntitledummies=pd.get_dummies(df_train[['Title']], prefix_sep='_') #Title\ndf_train = pd.concat([df_train, titledummies], axis=1) \nttitledummies=pd.get_dummies(df_test[['Title']], prefix_sep='_') #Title\ndf_test = pd.concat([df_test, ttitledummies], axis=1) \nprint('Title categories added')",
      "execution_count": 24,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "6e8ddee2-7992-47e4-acaf-ada16fc2f3fb",
        "_uuid": "ce0d55f3dc2247dd4403522bedf6ede3c9929c51"
      },
      "cell_type": "markdown",
      "source": "## Title Cetegory"
    },
    {
      "metadata": {
        "_cell_guid": "25b58c33-0bb0-4140-b5d2-00db768a0847",
        "_uuid": "1841f1df481ac13a11bc86683bef65b53d49afd0",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Mapping titles\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Millitary\": 5, \"Honor\": 6}\ndf_data[\"TitleCat\"] = df_data['Title'].map(title_mapping)\ndf_data[\"TitleCat\"] = df_data[\"TitleCat\"].astype(int)\ndf_train[\"TitleCat\"] = df_data[\"TitleCat\"][:891]\ndf_test[\"TitleCat\"] = df_data[\"TitleCat\"][891:]\nprint('Title Category created')",
      "execution_count": 25,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "5b65087f-6961-48cc-aefe-0deb4a9b1e83",
        "_uuid": "2bb8ee7bb6ee3f6bc3a9f5fc5d3758f40396ca4e"
      },
      "cell_type": "markdown",
      "source": "## Fill age based on title\n\nThe Visualisations of age by title suggests that if  we  create our age estimate by looking at the passengers title and using the average age for that title it may produce a more accurate estimate.  "
    },
    {
      "metadata": {
        "_cell_guid": "65f34a17-e349-4f58-a0de-5bbdb8516c4d",
        "_uuid": "4c138ced404146c2b4bd2f0e5fdf3d9cd7701531",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "titles = ['Master', 'Miss', 'Mr', 'Mrs', 'Millitary','Honor']\nfor title in titles:\n    age_to_impute = df_data.groupby('Title')['Age'].median()[title]\n    df_data.loc[(df_data['Age'].isnull()) & (df_data['Title'] == title), 'Age'] = age_to_impute\n# Age in df_train and df_test:\ndf_train[\"Age\"] = df_data['Age'][:891]\ndf_test[\"Age\"] = df_data['Age'][891:]\nprint('Missing Ages Estimated')",
      "execution_count": 26,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "005d5aa4-3bee-48ab-b3b4-17ba672a56a1",
        "_uuid": "294626f5dc7d074884a1234a07ca0516a647a8c0",
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "## Create Agebands"
    },
    {
      "metadata": {
        "_cell_guid": "bf8e352d-a753-420a-a172-7a77a6348c6e",
        "_uuid": "d91733b91b7787e5562c21b7d3a1a6061a492edc",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#map Fare values into groups of numerical values\ndf_data[\"AgeBand\"] = pd.qcut(df_data['Age'], 8, labels = [1, 2, 3, 4,5,6,7,8]).astype('int')\ndf_train[\"AgeBand\"] = pd.qcut(df_train['Age'], 8, labels = [1, 2, 3, 4,5,6,7,8]).astype('int')\ndf_test[\"AgeBand\"] = pd.qcut(df_test['Age'], 8, labels = [1, 2, 3, 4,5,6,7,8]).astype('int')\ndf_train[[\"AgeBand\", \"Survived\"]].groupby([\"AgeBand\"], as_index=False).mean()\nprint('AgeBand feature created')",
      "execution_count": 27,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "278791f7-2e30-4e9e-aced-7646350993e7",
        "_uuid": "88c9f8d7fb1a843d36fbb7f60c19d2bfcb57364e",
        "collapsed": true
      },
      "cell_type": "markdown",
      "source": "## Visualize Age Data"
    },
    {
      "metadata": {
        "_cell_guid": "46310750-e237-42ed-8fb4-8e397ed8021e",
        "_uuid": "b43d90e1336791cbb92814d9c76b1038f3efc2e1",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Visualise Age Data \nfig, (axis1,axis2) = plt.subplots(1,2,figsize=(15,4))\naxis1.set_title('Training Age values - Titanic')\naxis2.set_title('Test Age values - Titanic')\n\n# plot original Age values\ndf_train['Age'].dropna().astype(int).hist(bins=70, ax=axis1)\n#df_test['Age'].dropna().astype(int).hist(bins=70, ax=axis1)\n        \n# plot new Age Values\n#df_train['Age'].hist(bins=70, ax=axis2)\ndf_test['Age'].hist(bins=70, ax=axis2)\n\n# peaks for survived/not survived passengers by their age\nfacet = sns.FacetGrid(df_train, hue=\"Survived\",palette = 'seismic',aspect=4)\nfacet.map(sns.kdeplot,'Age',shade= True)\nfacet.set(xlim=(0, df_train['Age'].max()))\nfacet.add_legend()",
      "execution_count": 28,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "b1d61706-8a04-472c-afcb-4c5bc1cff3b8",
        "_uuid": "4db2538017a5dffd13828396d702fc1466cb0c7f",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "sns.boxplot(data = df_train, x = \"Title\", y = \"Age\");",
      "execution_count": 29,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "7d1d8177-c410-40ac-bcc4-3a8a7655a607",
        "_uuid": "b8bd16e435f03db3e0be3e0e38b4d011c571dc98"
      },
      "cell_type": "markdown",
      "source": "## Lone Travellers Feature "
    },
    {
      "metadata": {
        "_cell_guid": "a5e8b97a-c1a0-46a3-ab72-20c9f316fe39",
        "_uuid": "94804f474da36936590adebe2bd836c3425dbc05",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "df_train[\"Alone\"] = np.where(df_train['SibSp'] + df_train['Parch'] + 1 == 1, 1,0) # People travelling alone\ndf_test[\"Alone\"] = np.where(df_test['SibSp'] + df_test['Parch'] + 1 == 1, 1,0) # People travelling alone\nprint('Lone traveller feature created')",
      "execution_count": 30,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "48736798-d8b0-4c4f-80ea-c767ddf45cb3",
        "_uuid": "bdc2b37c4ed4794c74e5e1e8a8be30cd0482ad1a"
      },
      "cell_type": "markdown",
      "source": "## Family Size Feature"
    },
    {
      "metadata": {
        "_cell_guid": "af0da412-4654-402e-a7cf-66f06a2c26f0",
        "_uuid": "b28edbd85b8a8f9711629ce170051e79c11fbff1",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "df_train[\"Family Size\"] = (df_train['SibSp'] + df_train['Parch'] + 1)\ndf_test[\"Family Size\"] = df_test['SibSp'] + df_test['Parch'] + 1\nprint('Family size feature created')",
      "execution_count": 31,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "f4f8b2cf-5e1f-4516-96fa-b4684b54ea93",
        "_uuid": "5ef2960e91f503ce9ae5ef96f4b04909f74d6bd6"
      },
      "cell_type": "markdown",
      "source": "## Family Survival\n\nThis is based on code taken from from https://www.kaggle.com/shunjiangxu/blood-is-thicker-than-water-friendship-forever"
    },
    {
      "metadata": {
        "_cell_guid": "d2c8840b-3550-4905-ba71-df5bc9567892",
        "_uuid": "efd286c6c48091714b8a51d6aa4a4577c5b9bdfd",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# get last name\ndf_data[\"Last_Name\"] = df_data['Name'].apply(lambda x: str.split(x, \",\")[0])\n# Set survival value\nDEFAULT_SURVIVAL_VALUE = 0.5\ndf_data[\"Family_Survival\"] = DEFAULT_SURVIVAL_VALUE\n\n# Find Family groups by Fare\nfor grp, grp_df in df_data[['Survived','Name', 'Last_Name', 'Fare', 'Ticket', 'PassengerId',\n                           'SibSp', 'Parch', 'Age', 'Cabin']].groupby(['Last_Name', 'Fare']):\n    \n    if (len(grp_df) != 1):\n        # A Family group is found.\n        for ind, row in grp_df.iterrows():\n            smax = grp_df.drop(ind)['Survived'].max()\n            smin = grp_df.drop(ind)['Survived'].min()\n            passID = row['PassengerId']\n            if (smax == 1.0):\n                df_data.loc[df_data['PassengerId'] == passID, 'Family_Survival'] = 1\n            elif (smin==0.0):\n                df_data.loc[df_data['PassengerId'] == passID, 'Family_Survival'] = 0\n\nprint(\"Number of passengers with family survival information:\", \n      df_data.loc[df_data['Family_Survival']!=0.5].shape[0])\n\n# Find Family groups by Ticket\nfor _, grp_df in df_data.groupby('Ticket'):\n    if (len(grp_df) != 1):\n        for ind, row in grp_df.iterrows():\n            if (row['Family_Survival'] == 0) | (row['Family_Survival']== 0.5):\n                smax = grp_df.drop(ind)['Survived'].max()\n                smin = grp_df.drop(ind)['Survived'].min()\n                passID = row['PassengerId']\n                if (smax == 1.0):\n                    df_data.loc[df_data['PassengerId'] == passID, 'Family_Survival'] = 1\n                elif (smin==0.0):\n                    df_data.loc[df_data['PassengerId'] == passID, 'Family_Survival'] = 0\n                        \nprint(\"Number of passenger with family/group survival information: \" \n      +str(df_data[df_data['Family_Survival']!=0.5].shape[0]))\n\n# Family_Survival in df_train and df_test:\ndf_train[\"Family_Survival\"] = df_data['Family_Survival'][:891]\ndf_test[\"Family_Survival\"] = df_data['Family_Survival'][891:]",
      "execution_count": 32,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "0069ade8-2b34-4be3-8ebe-a29f6b396283",
        "_uuid": "821395bd7d2b3435f2476723067d04fdeaf1a31d"
      },
      "cell_type": "markdown",
      "source": "## Cabin feature"
    },
    {
      "metadata": {
        "_cell_guid": "cdf5b923-4275-4bb8-a812-58e385c67184",
        "_uuid": "9106eaa1504c1be04bc5155fb79711f50e70a477",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# check if cabin inf exists\ndf_data[\"HadCabin\"] = (df_data[\"Cabin\"].notnull().astype('int'))\n# split Embanked into df_train and df_test:\ndf_train[\"HadCabin\"] = df_data[\"HadCabin\"][:891]\ndf_test[\"HadCabin\"] = df_data[\"HadCabin\"][891:]\nprint('Cabin feature created')",
      "execution_count": 33,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "d116a6c7-13e4-49c4-93f9-ebea7a24900f",
        "_uuid": "1971f3e584336c047064e7aabf255d1740604076"
      },
      "cell_type": "markdown",
      "source": "## Deck feature"
    },
    {
      "metadata": {
        "_cell_guid": "5e5c0afe-d43d-40a6-a7ad-305a9c2e6921",
        "_uuid": "1c51d71226a432968085d34b707979fe3c71c2f7",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Extract Deck\ndf_data[\"Deck\"] = df_data.Cabin.str.extract('([A-Za-z])', expand=False)\ndf_data[\"Deck\"] = df_data[\"Deck\"].fillna(\"N\")\n# Map Deck\ndeck_mapping = {\"N\":0,\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5}\ndf_data['Deck'] = df_data['Deck'].map(deck_mapping)\n#Split to training and test\ndf_train[\"Deck\"] = df_data[\"Deck\"][:891]\ndf_test[\"Deck\"] = df_data[\"Deck\"][891:]\nprint('Deck feature created')",
      "execution_count": 34,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "127a8ac6-0c34-445d-95d5-a772f8c09d59",
        "_uuid": "ab9fdd9f1f291415911a6391a371fc787f76794d"
      },
      "cell_type": "markdown",
      "source": "# Exploring the Engineered data"
    },
    {
      "metadata": {
        "_cell_guid": "a93abe26-c2af-43b5-a54a-c783a612a931",
        "_uuid": "5f1b6ffb552a69c959a609840fa868baa69f9650"
      },
      "cell_type": "markdown",
      "source": "## Missing Data"
    },
    {
      "metadata": {
        "_cell_guid": "c51d7f61-2358-4ea6-8b46-fda58f9135fc",
        "_uuid": "2cd217d4a38e136a9185c5311cd920355d40323e",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#check for any other unusable values\nprint(pd.isnull(df_test).sum())",
      "execution_count": 35,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "b510aa95-ace2-46c1-a45e-bfef2bd966cc",
        "_uuid": "4b6d1483c18378c32e1f88f56836232dbe341863"
      },
      "cell_type": "markdown",
      "source": "## Statistical Overview"
    },
    {
      "metadata": {
        "_cell_guid": "ed4cdefb-de12-4f7b-98e5-b532bc2248ae",
        "_uuid": "db20ccbf0db2f0c3f47a31b7e759a80dbbcce155",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "df_train.describe()",
      "execution_count": 36,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "1221771d-4505-4865-8efb-34bd9b643133",
        "_uuid": "f264ea422251e2c02721cd954e2df78c2006ad6a"
      },
      "cell_type": "markdown",
      "source": "# Visualizing age data\nWe could estimate all of the ages based on the mean and standard deviation of the data set, however as age is obviously an important feature in pridicting survival and we need to look at the other features and see if we can work out a way to make a more accurate estimate of age for any given passenger. First lets look at the different age distributions of passengers by title."
    },
    {
      "metadata": {
        "_cell_guid": "16cbd148-bba4-479c-b12e-fd8e4f3d601d",
        "_uuid": "6316a37319e860411d240ee58e73fe977157a45b",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Groupby title\ndf_train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()\n# plot age distribution by title\nfacet = sns.FacetGrid(data = df_train, hue = \"Title\", legend_out=True, size = 5)\nfacet = facet.map(sns.kdeplot, \"Age\")\nfacet.add_legend();",
      "execution_count": 37,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "7482e53e-c034-42a0-85aa-9a657e90c20e",
        "_uuid": "aad62a7b5e958339fa3daf0ef666bbad8d0abfea"
      },
      "cell_type": "markdown",
      "source": "The age distribution looks slightly suspect and possibly merits further investigation, for example while master generally refers to male's under 16 there a number that are over 40, this might be explained if master is also a title in nautical terms like 'Master Seaman'. You might also expect a quite Normal distribution of ages for any given title, but in many cases this doesn't seem to be the case, this is most likely caused by out estimated numbers skewing the data, one way to avoid this would be to use a random number based on the standard deviation in the estimate for each to get a more natural dataset. We could also use age bands rather than age in the model."
    },
    {
      "metadata": {
        "_cell_guid": "6f24c55a-74e1-4789-b430-ebba2d2a234f",
        "_uuid": "cda5452db331d74e319c95053249adc2da8d4477"
      },
      "cell_type": "markdown",
      "source": "### Survival by FareBand and Gender "
    },
    {
      "metadata": {
        "_cell_guid": "34dc8d59-7425-492e-bb37-8d6856b27536",
        "_uuid": "2ab0e04ade741513a4a7792e6603ee82ef9a264c",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "grid = sns.FacetGrid(df_train, col = \"FareBand\", row = \"Sex\", hue = \"Survived\", palette = 'seismic')\ngrid = grid.map(plt.scatter, \"PassengerId\", \"Age\")\ngrid.add_legend()\ngrid",
      "execution_count": 38,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "2ba192cc-6293-4050-a742-dcaa67b6416e",
        "_uuid": "2108bea0ee1fc9d83ad31a1a11f668bf2e24ee38"
      },
      "cell_type": "markdown",
      "source": "### Survival by Deck and Gender "
    },
    {
      "metadata": {
        "_cell_guid": "531add0d-4a07-4247-bda2-d07e42b85bef",
        "_uuid": "5e0b89f06bff8b5d647e6eb3743abc132c956d62",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "grid = sns.FacetGrid(df_train, col = \"Deck\", row = \"Sex\", hue = \"Survived\", palette = 'seismic')\ngrid = grid.map(plt.scatter, \"PassengerId\", \"Age\")\ngrid.add_legend()\ngrid",
      "execution_count": 39,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "c6b6a59d-0d55-44f9-a5e4-3dbc78dd213b",
        "_uuid": "05756f61c82449b0a2804b940bc5c9b305e39040"
      },
      "cell_type": "markdown",
      "source": "### Survival by Family Size and Gender "
    },
    {
      "metadata": {
        "_cell_guid": "14a51b8e-7eea-49c3-91db-59aed6326ff2",
        "_uuid": "10b4e8ce607a6297500cc45ae72e1d1a59861bf4",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "grid = sns.FacetGrid(df_train, col = \"Family Size\", row = \"Sex\", hue = \"Survived\", palette = 'seismic')\ngrid = grid.map(plt.scatter, \"PassengerId\", \"Age\")\ngrid.add_legend()\ngrid",
      "execution_count": 40,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "63efa126-2a01-4a30-8020-f1aa48110f06",
        "_uuid": "58021ef05643a0238b8d0d5432a7af4d9da7c357"
      },
      "cell_type": "markdown",
      "source": "### Survival by Passenger Class and Family Size"
    },
    {
      "metadata": {
        "_cell_guid": "5da3b70d-9889-4ce4-9ab7-999052e02b7b",
        "_uuid": "5862c89f842661813d0a9a3c55b1ace0d990ad2d",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "x1=df_train[df_train[\"Survived\"]==0]\nx2=df_train[df_train[\"Survived\"]==1]\n\n# Set up the matplotlib figure\nplt.figure(1)\nsns.jointplot(x=\"Family Size\", y=\"Pclass\", data=x1, kind=\"kde\", color='b');\nplt.figure(2)\nsns.jointplot(x=\"Family Size\", y=\"Pclass\", data=x2, kind=\"kde\", color='r');\nplt.show()",
      "execution_count": 41,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "910ee13d-7181-4e07-9332-7fe0d4c96024",
        "_uuid": "77d6372808e34a2931e3ffac62880a590b0b1558"
      },
      "cell_type": "markdown",
      "source": "### Fare Jointplot "
    },
    {
      "metadata": {
        "_cell_guid": "90ba8a16-086c-4664-a812-a09e7755c1d1",
        "_uuid": "2fa46d8f2cd4229152dd1bf37bcaf4dbcd1aedf8",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "plt.figure(3)\nsns.jointplot(data=x1, x='PassengerId', y='Age', kind='scatter',color='b')\nplt.figure(4)\nsns.jointplot(data=x2, x='PassengerId', y='Age', kind='scatter',color='r')\n\n# sns.plt.show()",
      "execution_count": 43,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "9eb86bf1-2838-4aeb-8d63-21e2ec707dca",
        "_uuid": "70b77d246c62e356a9521f8602c844f8ac59c73b"
      },
      "cell_type": "markdown",
      "source": "# Re-train the model on new features"
    },
    {
      "metadata": {
        "_cell_guid": "cc55e96d-a719-4383-a1d5-9d1504733201",
        "_uuid": "493de5388b78f8a2b12b9d79ab9fbbadad7b9ad0",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "df_train.head()",
      "execution_count": 44,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "afe77855-d9f3-47ec-af05-0da48baa7656",
        "_uuid": "3d597ed6199a01efefeb4d36848c2f321a9a622d"
      },
      "cell_type": "markdown",
      "source": "# Re-evaluate the on new features"
    },
    {
      "metadata": {
        "_cell_guid": "aaa2c9cc-234e-455a-bb25-339d5655a286",
        "_uuid": "560b663f936650b7a6c2c844eac395f5de41dd23",
        "scrolled": true,
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Re-evaluate with new features\n\nNUMERIC_COLUMNS=['Alone','Family Size','Sex','Pclass','Fare','FareBand','Age','TitleCat','Embarked'] #72\nREVISED_NUMERIC_COLUMNS=['Pclass','Age','SibSp','Parch','Family_Survival','Alone','Sex_female','Sex_male','Title_Master', 'Title_Miss','Title_Mr', 'Title_Mrs', 'Title_Millitary','Embarked'] #84\n\n# create test and training data\ndata_to_train = df_train[REVISED_NUMERIC_COLUMNS].fillna(-1000)\nX_train, X_test, y_train, y_test = train_test_split(data_to_train, df_train['Survived'], test_size=0.3,random_state=21, stratify=df_train['Survived'])\n\nRandomForest = RandomForestClassifier(random_state = 0)\nRandomForest.fit(X_train, y_train)\nprint('Evaluation complete')",
      "execution_count": 45,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "46625816-d7d8-4ab8-8188-b476c446a853",
        "_kg_hide-input": true,
        "_kg_hide-output": true,
        "_uuid": "4ae404d5a825a31e6d559e23290d7b9990565fbf",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Print the accuracy# Print  \nprint(\"Accuracy: {}\".format(RandomForest.score(X_test, y_test)))",
      "execution_count": 46,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "d95e4667-d810-46d3-a832-e781e58fdbc0",
        "_uuid": "9faa0d437e4c7943e1639d0acfb33780a8b2b24b"
      },
      "cell_type": "markdown",
      "source": "## Feature Importance"
    },
    {
      "metadata": {
        "_cell_guid": "737ac635-c4d8-48a4-858e-797d803b2c14",
        "_uuid": "71d23b03f840b1b757245bb3c1aa0641e49eefdd",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "RandomForest_checker = RandomForestClassifier()\nRandomForest_checker.fit(X_train, y_train)\nimportances_df = pd.DataFrame(RandomForest_checker.feature_importances_, columns=['Feature_Importance'],\n                              index=X_train.columns)\nimportances_df.sort_values(by=['Feature_Importance'], ascending=False, inplace=True)\nprint(importances_df)",
      "execution_count": 47,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "4dc3421e-f916-44a6-997f-0fcd7a294075",
        "_uuid": "11617efd02c144508d9e4feaed0e204009869c83"
      },
      "cell_type": "markdown",
      "source": "## Feature Correlation"
    },
    {
      "metadata": {
        "_cell_guid": "45d1ce22-ecf9-4ed7-a1af-6e30c9b49452",
        "_uuid": "faa93fd2f97a2c0c129b34da798b3e87d60eba5d",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#map feature correlation\nf,ax = plt.subplots(figsize=(12, 12))\npal = sns.dark_palette(\"blue\", as_cmap=True)\nsns.heatmap(df_train.corr(),annot=True,cmap=pal, linewidths=.5, fmt= '.1f',ax=ax)",
      "execution_count": 49,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "1ea361c4-4126-4e9d-b548-ea56ef5ae083",
        "_uuid": "f8195440d641ac964d496f1907bb54f64da5a0be"
      },
      "cell_type": "markdown",
      "source": "# Re-forcast predictions based on new features"
    },
    {
      "metadata": {
        "_cell_guid": "755a1493-f4be-40ae-be33-9ee302882a6f",
        "_uuid": "9fdcd905b7a60429ceb4d62620c388a9faa980a7",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "test = df_test[REVISED_NUMERIC_COLUMNS].fillna(-1000)\nSubmission['Survived']=RandomForest.predict(test)\nprint(Submission.head())\nprint('Submission created')",
      "execution_count": 50,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "0d689a14-d161-407b-9fbe-0545d654da7e",
        "_uuid": "50f117aaa9560e10383d0a4b0d891675c0d328a2"
      },
      "cell_type": "markdown",
      "source": "# Make revised submission"
    },
    {
      "metadata": {
        "_cell_guid": "f61ee101-471d-4425-9a95-a10e1b269bdf",
        "_uuid": "8971da22e3035e23af2f7b587dcdab60a126d1b9",
        "collapsed": true,
        "scrolled": false,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# write data frame to csv file\n# Submission.set_index('PassengerId', inplace=True)\n#Submission.to_csv('finalrevised01.csv',sep=',')\nprint('file created')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "68850cd4-c427-4f53-8af6-a7f2c9a9df1b",
        "_uuid": "73fb67722c4b233ff29e491955b752f99eb2a539"
      },
      "cell_type": "markdown",
      "source": "The second revised submission scored 0.75598 which was an improvement of the original revision which scored 0.64593, this used was  is an improvement on the original score of 0.57894. This advanced the submission to 9117 place on the leaderboard, from the starting point of 10599th place! Obviousy a step in the right direction but still needing work."
    },
    {
      "metadata": {
        "_cell_guid": "b4680f97-d9e3-40d8-b5b3-39d914e25965",
        "_uuid": "f3425a2b3b51e62d06f9c7550b485ef25b3c7539"
      },
      "cell_type": "markdown",
      "source": "# Stage 3 : Test Different Models and parameters"
    },
    {
      "metadata": {
        "_cell_guid": "b506b4f4-e3c2-43a4-a070-75822e288fe7",
        "_uuid": "443ea01c565176c5b6c059653b680651314f0de9"
      },
      "cell_type": "markdown",
      "source": "## Split data into test and training"
    },
    {
      "metadata": {
        "_cell_guid": "73f4510e-9e33-48b7-96f4-08842da93099",
        "_uuid": "1036293295a5a355f3768ad7fa88457ff8122320",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\nNUMERIC_COLUMNS=['Alone','Family Size','Sex','Pclass','Fare','Age','TitleCat','FareBand','Embarked']\nREVISED_NUMERIC_COLUMNS=['Pclass','Age','SibSp','Parch','Family_Survival','Alone','Sex_female','Sex_male','Title_Master', 'Title_Miss','Title_Mr', 'Title_Mrs', 'Title_Millitary','Embarked'] #84\n\n# create test and training data\ndata_to_train = df_train[REVISED_NUMERIC_COLUMNS].fillna(-1000)\nprediction = df_train[\"Survived\"]\nX_train, X_val, y_train, y_val = train_test_split(data_to_train, prediction, test_size = 0.3,random_state=21, stratify=y)\nprint('Data split')",
      "execution_count": 51,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "bf4caf90-bb98-4fe9-9722-c3fe88bb1a13",
        "_uuid": "b962efd29e3547c04e01164161d4fdd0bf3d9cd3"
      },
      "cell_type": "markdown",
      "source": "## Support Vector Machines\n\nHas more flexibility in the choice of penalties and loss functions and should scale better to large numbers of samples.\n1. This class supports both dense and sparse input and the multiclass support is handled according to a one-vs-the-rest scheme."
    },
    {
      "metadata": {
        "_cell_guid": "cc0f0133-b9ec-4d0a-b85f-085b19aaad1c",
        "_uuid": "13b4555bf513117290dd190ac28fed8a7e4b942b",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "clf = SVC()\n\nclf.fit(X_train, y_train)\ny_pred = clf.predict(X_val)\nacc_clf = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_clf)",
      "execution_count": 52,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "a742b2d2-8a94-4414-973b-f3d15a42cf40",
        "_uuid": "71fd9ae20f920a75edb146d963a67db9f7c0c002"
      },
      "cell_type": "markdown",
      "source": "## LinearSVC"
    },
    {
      "metadata": {
        "_cell_guid": "ce5bd8c0-6ffd-4e35-89fc-62fdc95a29fb",
        "_uuid": "f2144f3e0e6f475dc72d33f8436edf170246bd4c",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Linear SVC\nfrom sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(X_train, y_train)\ny_pred = linear_svc.predict(X_val)\nacc_linear_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_linear_svc)",
      "execution_count": 53,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "56a58cc6-8538-419b-9bc6-84bd0443626d",
        "_uuid": "86cd38a1a6fba7114cc59182b80c4480c6d4de58"
      },
      "cell_type": "markdown",
      "source": "## Logistic Regression"
    },
    {
      "metadata": {
        "_cell_guid": "04e84c8c-3b0a-4a5f-bf4c-3be15e1cfef7",
        "_uuid": "d7fcc417445b5645dce8c36bf2bb02a4cffecbe9",
        "scrolled": false,
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_val)\nacc_logreg = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_logreg)",
      "execution_count": 54,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "2f1c6eed-6536-458a-b7e5-934ffde45a62",
        "_uuid": "5dd4fcac5736d6048dd7d47108b53335a7bef7f3"
      },
      "cell_type": "markdown",
      "source": "## K Nearest Neighbors"
    },
    {
      "metadata": {
        "_cell_guid": "4249ed43-d0ac-4564-80a2-e09f6cf446ee",
        "_uuid": "1afab69791259b2f145d0f533cf4eb6fc4e7ac82",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# KNN or k-Nearest Neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_val)\nacc_knn = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_knn)",
      "execution_count": 55,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "85b47396-750a-4a35-b415-acea360679d0",
        "_uuid": "f0150561749e08696a0c150fba42f439a1c07f3e"
      },
      "cell_type": "markdown",
      "source": "## Gaussian Naive Bayes"
    },
    {
      "metadata": {
        "_cell_guid": "6b1a74a3-2834-47ff-aee0-b2bfc68540fc",
        "_uuid": "d24400ad0fac6c28dddd9b6f0b71ceba8a39e653",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\n\ngaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\ny_pred = gaussian.predict(X_val)\nacc_gaussian = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gaussian)",
      "execution_count": 56,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "035469fb-4b8d-4d2d-a056-4498f7d4e815",
        "_uuid": "4f5be1e8410b2208897ae8df93f334ee63332805"
      },
      "cell_type": "markdown",
      "source": "## Decision Tree"
    },
    {
      "metadata": {
        "_cell_guid": "69e9aa91-4451-47ae-95db-e9c8da605311",
        "_uuid": "21cd723ecb8e0b3c517aa7681523ddde3dab0cd8",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "#Decision Tree\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecisiontree = DecisionTreeClassifier()\ndecisiontree.fit(X_train, y_train)\ny_pred = decisiontree.predict(X_val)\nacc_decisiontree = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_decisiontree)",
      "execution_count": 57,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "074abaa3-522d-43c7-a979-c370d52ae47f",
        "_uuid": "763494c08e2edddef19b49608da458774cb8515f"
      },
      "cell_type": "markdown",
      "source": "## Random Forest"
    },
    {
      "metadata": {
        "_cell_guid": "104a417c-d913-40fe-81d7-97f71a8db89a",
        "_uuid": "790e9ad10cc6cd242fa959ebec9cc63b9a485996",
        "scrolled": true,
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandomforest = RandomForestClassifier(random_state = 0)\nrandomforest.fit(X_train, y_train)\ny_pred = randomforest.predict(X_val)\nacc_randomforest = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_randomforest)",
      "execution_count": 58,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "9551bb30-3508-4d35-81c2-087a5266636c",
        "_uuid": "9647c2ca262707f3c3a82c1a7f0040eed862a977"
      },
      "cell_type": "markdown",
      "source": "## Gradient Boosting "
    },
    {
      "metadata": {
        "_cell_guid": "88a917f8-e75d-48e8-af76-5ee7da57f9dd",
        "_uuid": "64b2374dd1d3ffab3b63085cd9a8dcb9b01b3eec",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngbk = GradientBoostingClassifier()\ngbk.fit(X_train, y_train)\ny_pred = gbk.predict(X_val)\nacc_gbk = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gbk)",
      "execution_count": 59,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "5bc8d24b-28cb-4149-bc24-075b9b40704a",
        "_uuid": "4c4d42523a98602179eaafd80827069e880e1532"
      },
      "cell_type": "markdown",
      "source": "## Extra Trees"
    },
    {
      "metadata": {
        "_cell_guid": "ad454583-17b9-4742-9505-0bf74391f260",
        "_uuid": "e11760ba734d1cbc8fb3011e6211aee25069cfbe",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "ExtraTreesClassifier\n# Gradient Boosting Classifier\nfrom sklearn.ensemble import ExtraTreesClassifier\n\net = ExtraTreesClassifier()\net.fit(X_train, y_train)\ny_pred = et.predict(X_val)\nacc_et = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_et)",
      "execution_count": 60,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "d66a6c84-133f-4600-83bc-dde061dcd523",
        "_uuid": "8ec249236702ac1e14d9eca31bf9663666b531fa"
      },
      "cell_type": "markdown",
      "source": "## Stochastic Gradient Descent"
    },
    {
      "metadata": {
        "_cell_guid": "e422b7cc-f6f8-4a35-8de4-119b11409dbe",
        "_uuid": "549cc6b5b02f76aab75a3b025005b40876401a33",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Stochastic Gradient Descent\nfrom sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier()\nsgd.fit(X_train, y_train)\ny_pred = sgd.predict(X_val)\nacc_sgd = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_sgd)",
      "execution_count": 61,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "89bfe038-5848-48f1-a15c-26d3754ef78f",
        "_uuid": "552db23cb684cd76af3ea10ab23fce512ed5ae3c"
      },
      "cell_type": "markdown",
      "source": "### Perceptron"
    },
    {
      "metadata": {
        "_cell_guid": "d8e91b01-3d86-44f6-adc8-bd7e8fdc9c2a",
        "_uuid": "7e3ecb050671aa14f75fc60ec86ad522f09e2877",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Perceptron\nfrom sklearn.linear_model import Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(X_train, y_train)\ny_pred = perceptron.predict(X_val)\nacc_perceptron = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_perceptron)",
      "execution_count": 62,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "00f8e497-d81b-4a58-9540-cfd55303a973",
        "_uuid": "b17e4b5b04a14ae84f7f0b1a34d951d44876fe58"
      },
      "cell_type": "markdown",
      "source": "## xgboost"
    },
    {
      "metadata": {
        "_cell_guid": "f82f70dd-912b-4ec3-9dac-fd63aa72f27f",
        "_uuid": "a6647116fc05b74500095b33d98810377ca60a4a",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# xgboost\nfrom xgboost import XGBClassifier\n\nxgb = XGBClassifier(n_estimators=10)\nxgb.fit(X_train, y_train)\ny_pred = xgb.predict(X_val)\nacc_xgb = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_xgb)",
      "execution_count": 63,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "29196c0a-5f90-4bdb-88bb-72ad07c12732",
        "_uuid": "e8fadb4412e7274f4fe91daf9f5c899beb9ac80d"
      },
      "cell_type": "markdown",
      "source": "## Comparing the results"
    },
    {
      "metadata": {
        "_cell_guid": "16e75897-65af-40c4-b236-ea0a3c42c196",
        "_uuid": "476d4fe076dd263d96070206bb616b2549133a15",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Linear SVC', \n              'Decision Tree', 'Gradient Boosting Classifier','Extra Trees','Stochastic Gradient Descent','Perceptron','xgboost'],\n    'Score': [acc_clf, acc_knn, acc_logreg, \n              acc_randomforest, acc_gaussian,acc_linear_svc, acc_decisiontree,\n             acc_gbk,acc_et,acc_sgd,acc_perceptron,acc_xgb]})\nmodels.sort_values(by='Score', ascending=False)",
      "execution_count": 65,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "e74f1aed-5b01-4a50-a078-2fe91fb4be84",
        "_uuid": "c75a06d718bf4e91ab977472870fc922736f75f3"
      },
      "cell_type": "markdown",
      "source": "# Reforcast predictions based on best performing model"
    },
    {
      "metadata": {
        "_cell_guid": "7b41f3e8-d98f-44b2-bb76-104a0a2171b7",
        "_uuid": "5a629fec076a8f4a27fc2a785edf4cc728ccc93a",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "test = df_test[REVISED_NUMERIC_COLUMNS].fillna(-1000)\n\nSubmission['Survived']=xgb.predict(test)\nprint(Submission.head(5))\nprint('Prediction complete')",
      "execution_count": 66,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "0fd0b448-227e-492d-89a3-9cdd5389f34b",
        "_uuid": "d32f3275adef82a72a3d7f237f923c8f6ee35d07"
      },
      "cell_type": "markdown",
      "source": "# Make model submission"
    },
    {
      "metadata": {
        "_cell_guid": "7bc66dd8-9b1a-4c85-9aa1-37cc03fe930f",
        "_uuid": "041de4936c148b80686c0e7b346bdd70d0c522b1",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# write data frame to csv file\n#Submission.set_index('PassengerId', inplace=True)\n#Submission.to_csv('finalsubmission.csv',sep=',')\n#Submission.to_csv('xgbbasicsubmission01.csv',sep=',')\nprint('File created')",
      "execution_count": 69,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "3099cd36-3434-46b4-a78e-585875de5b0a",
        "_uuid": "e32f127a2a97f13730c1302b8b595b2452e78045"
      },
      "cell_type": "markdown",
      "source": "# Stage 4 : Hyper Tuning the Models"
    },
    {
      "metadata": {
        "_cell_guid": "1df044b1-f8e6-485e-b1cd-9179c8a04595",
        "_uuid": "dda6888232a077b2da3c2b0ac93e722783327386",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import train_test_split\nNUMERIC_COLUMNS=['Pclass','Age','SibSp','Parch','Sex_female','Sex_male','Title_Master', 'Title_Miss','Title_Mr', 'Title_Mrs', 'Title_Services','FareBand','Embarked','Alone','Family Size']\nREVISED_NUMERIC_COLUMNS=['Pclass','AgeBand','SibSp','Parch','Family_Survival','Family Size','Alone','Sex_female','Sex_male','Title_Master', 'Title_Miss','Title_Mr', 'Title_Mrs', 'Title_Millitary','Embarked'] #84\n\n# create test and training data\ndata_to_train = df_train[REVISED_NUMERIC_COLUMNS].fillna(-1000)\nprediction = df_train[\"Survived\"]\nX_train, X_val, y_train, y_val = train_test_split(data_to_train, prediction, test_size = 0.3,random_state=21, stratify=prediction)\nprint('Data Split')",
      "execution_count": 70,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "635b8fd0-3e7b-46b1-bc95-33fe67b36eef",
        "_uuid": "8350b77da7bc61bedc4374f308e09db22ba6df41"
      },
      "cell_type": "markdown",
      "source": "\n"
    },
    {
      "metadata": {
        "_cell_guid": "09934873-663b-4e24-b589-6f9009d4fa5f",
        "_uuid": "57c4ff97a9449b31e46b75129ebd0e67bec19c9f"
      },
      "cell_type": "markdown",
      "source": "## Linear Regression SVC"
    },
    {
      "metadata": {
        "_cell_guid": "29cccffb-38c3-40f3-8adb-55bb9e55f3e4",
        "_uuid": "9eb1635f91e2beb04412c47a6a07a6d141778309",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import GridSearchCV\n\n# Support Vector Classifier parameters \nparam_grid = {'C':np.arange(1, 7),\n              'degree':np.arange(1, 7),\n              'max_iter':np.arange(0, 12),\n              'kernel':['rbf','linear'],\n              'shrinking':[0,1]}\n\nclf = SVC()\nsvc_cv=GridSearchCV(clf, param_grid, cv=10)\nsvc_cv.fit(X_train, y_train)\n\nprint(\"Tuned SVC Parameters: {}\".format(svc_cv.best_params_))\nprint(\"Best score is {}\".format(svc_cv.best_score_))\nacc_svc_cv = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_svc_cv)",
      "execution_count": 73,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "d3e7fd18-76fa-47cf-bdf6-8c59edc7a46a",
        "_uuid": "acb983e1342ac4f56181a80a8fd53deb28b4729c"
      },
      "cell_type": "markdown",
      "source": "## Logistic Regression"
    },
    {
      "metadata": {
        "_cell_guid": "11836eb4-654d-43a4-a6ad-784992ef2cae",
        "_uuid": "0a9d6988a490b93d3c4fa085b6fc326153288b5a",
        "scrolled": true,
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\n\n# create parameter grid as a dictionary where the keys are the hyperparameter names and the values are lists of values that we want to try.\nparam_grid = {\"solver\": ['newton-cg','lbfgs','liblinear','sag','saga'],'C': [0.01, 0.1, 1, 10, 100]}\n\n# instanciate classifier\nlogreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\n\nlogreg_cv = GridSearchCV(logreg, param_grid, cv=30)\nlogreg_cv.fit(X_train, y_train)\n\ny_pred = logreg_cv.predict(X_val)\nprint(\"Tuned Logistic Regression Parameters: {}\".format(logreg_cv.best_params_))\nprint(\"Best score is {}\".format(logreg_cv.best_score_))\nacc_logreg_cv = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_logreg_cv)",
      "execution_count": 72,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "04a4fb0d-eeb2-4279-bc54-21109f78db3c",
        "_uuid": "637dcc57d021d4e98bcd1acec7153fc968c1cc28"
      },
      "cell_type": "markdown",
      "source": "## KNN "
    },
    {
      "metadata": {
        "_cell_guid": "ec9d9407-4ee9-47a6-893b-65dd2a5159c5",
        "_uuid": "b4c2895af86e90da19b5c4ee2b2df24d4f6f7275",
        "scrolled": true,
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# KNN or k-Nearest Neighbors with GridSearch\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import GridSearchCV\n\n# create parameter grid as a dictionary where the keys are the hyperparameter names and the values are lists of values that we want to try.\nparam_grid = {\"n_neighbors\": np.arange(1, 50),\n             \"leaf_size\": np.arange(20, 40),\n             \"algorithm\": [\"ball_tree\",\"kd_tree\",\"brute\"]\n             }\n# instanciate classifier\nknn = KNeighborsClassifier()\nknn.fit(X_train, y_train)\nknn_cv = GridSearchCV(knn, param_grid, cv=10)\nknn_cv.fit(X_train, y_train)\ny_pred = knn_cv.predict(X_val)\nprint(\"Tuned knn Parameters: {}\".format(knn_cv.best_params_))\nprint(\"Best score is {}\".format(knn_cv.best_score_))\nacc_knn_cv = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_knn_cv)",
      "execution_count": 74,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "7e923bac-35be-45c1-b6cb-2e53fec7b86c",
        "_uuid": "7d8265bd2bfa6b1db4cbaf28614c623f44c94179"
      },
      "cell_type": "markdown",
      "source": "## DecisionTree with RandomizedSearch"
    },
    {
      "metadata": {
        "_cell_guid": "96aa7acc-4a15-4ed9-862a-2fe9d053882b",
        "_uuid": "b65ae78b1dc789edf9d9ca9f4845fd84a2f5e45a",
        "scrolled": true,
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# DecisionTree with RandomizedSearch\n\n# Import necessary modules\nfrom scipy.stats import randint\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Setup the parameters and distributions to sample from: param_dist\nparam_dist = {\"random_state\" :  np.arange(0, 10),\n              \"max_depth\": np.arange(1, 10),\n              \"max_features\": np.arange(1, 10),\n              \"min_samples_leaf\": np.arange(1, 10),\n              \"criterion\": [\"gini\",\"entropy\"]}\n\n# Instantiate a Decision Tree classifier: tree\ntree = DecisionTreeClassifier()\n\n# Instantiate the RandomizedSearchCV object: tree_cv\ntree_cv = RandomizedSearchCV(tree, param_dist, cv=30)\n\n# Fit it to the data\ntree_cv.fit(X_train,y_train)\ny_pred = tree_cv.predict(X_val)\n# Print the tuned parameters and score\nprint(\"Tuned Decision Tree Parameters: {}\".format(tree_cv.best_params_))\nprint(\"Best score is {}\".format(tree_cv.best_score_))\nacc_tree_cv = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_tree_cv)",
      "execution_count": 76,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "0d32116b-5292-436a-8d86-1e93a21c3c76",
        "_uuid": "6617cd4820b68ba109c6485431bed2f3c74fc4a7"
      },
      "cell_type": "markdown",
      "source": "## Random Forrest"
    },
    {
      "metadata": {
        "_cell_guid": "a4fb80df-9bff-49cf-bc5c-05857954a6a9",
        "_uuid": "57ec97e39d8fc7350e7de3fca33ad0a07077d4c5",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Random Forrest\n\n# Import necessary modules\nfrom scipy.stats import randint\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Setup the parameters and distributions to sample from: param_dist\nparam_dist = {\"random_state\" :  np.arange(0, 10),\n              \"n_estimators\" :  np.arange(1, 20),\n              \"max_depth\": np.arange(1, 10),\n              \"max_features\": np.arange(1, 10),\n              \"min_samples_leaf\": np.arange(1, 10),\n              \"criterion\": [\"gini\",\"entropy\"]}\n\n# Instantiate a Decision Tree classifier: tree\nrandomforest = RandomForestClassifier()\n\n# Instantiate the RandomizedSearchCV object: tree_cv\nrandomforest_cv = RandomizedSearchCV(randomforest, param_dist, cv=30)\n\n# Fit it to the data\nrandomforest_cv.fit(X_train,y_train)\ny_pred = randomforest_cv.predict(X_val)\n# Print the tuned parameters and score\nprint(\"Tuned Decision Tree Parameters: {}\".format(randomforest_cv.best_params_))\nprint(\"Best score is {}\".format(randomforest_cv.best_score_))\nacc_randomforest_cv = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_randomforest_cv)",
      "execution_count": 77,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "a0d9a1a5-c205-43e8-bad4-b66dd27cea4d",
        "_uuid": "2e3e0cebc18761c6df40a92d0b0d36aec8893539"
      },
      "cell_type": "markdown",
      "source": "## Gradient Boosting"
    },
    {
      "metadata": {
        "_cell_guid": "2024cd8a-0e0f-4f95-9041-f8117d91283d",
        "_uuid": "880a91fc3d7019e480e47f85bc232fc6ce4b8951",
        "scrolled": true,
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.cross_validation import KFold;\n\n# Setup the parameters and distributions to sample from: param_dist\nparam_dist = {'max_depth':np.arange(1, 7),\n              'min_samples_leaf': np.arange(1, 6),\n              \"max_features\": np.arange(1, 10),\n             }\n\n# Instantiate Classifier\ngbk = GradientBoostingClassifier()\n\n# Instantiate the RandomizedSearchCV object: tree_cv\ngbk_cv = RandomizedSearchCV(gbk, param_dist, cv=30)\n\ngbk_cv.fit(X_train, y_train)\ny_pred = gbk_cv.predict(X_val)\n\nprint(\"Tuned Gradient Boost Parameters: {}\".format(gbk_cv.best_params_))\nprint(\"Best score is {}\".format(gbk_cv.best_score_))\nacc_gbk_cv = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gbk_cv)",
      "execution_count": 78,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "8a136cb4-f569-4fad-a55c-1dc335f4fa6f",
        "_uuid": "d3033d56687960b96b3343eb58290dc6b07c6d7a"
      },
      "cell_type": "markdown",
      "source": "## xgboost"
    },
    {
      "metadata": {
        "_cell_guid": "c348c87a-7dba-44f3-b46b-42648405eb65",
        "_uuid": "aa52db37764db2f4e930416d57d2449b2bfa775b",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# xgboost\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import RandomizedSearchCV\n\n# Setup the parameters and distributions to sample from: param_dist\nparam_dist = {'learning_rate': [.01, .03, .05, .1, .25], #default: .3\n            'max_depth': np.arange(1, 10), #default 2\n            'n_estimators': [10, 50, 100, 300], \n            'booster':['gbtree','gblinear','dart']\n            #'seed': 5  \n             }\n# Instantiate Classifier\nxgb = XGBClassifier()\n\n# Instantiate the RandomizedSearchCV object: tree_cv\nxgb_cv = RandomizedSearchCV(xgb, param_dist, cv=20)\n\n# Fit model\nxgb_cv.fit(X_train, y_train)\n\n# Make prediction\ny_pred = xgb_cv.predict(X_val)\n\n# Print results\nprint(\"xgBoost Parameters: {}\".format(xgb_cv.best_params_))\nprint(\"Best score is {}\".format(xgb_cv.best_score_))\nacc_xgb_cv = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_xgb_cv)",
      "execution_count": 79,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "fe0a6c92-ae79-4b23-9fd3-623d0605fafb",
        "_uuid": "86f584033cab06766198e953020bede5fe2759aa"
      },
      "cell_type": "markdown",
      "source": "## Comparing the results of the cross validated tuned models (best result)"
    },
    {
      "metadata": {
        "_cell_guid": "462195e0-3dad-497d-86d4-ac89eb75b462",
        "_uuid": "322232b262fd94c31225fa683ac76f5f706fe744",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "optmodels = pd.DataFrame({\n    'optModel': ['SVC','KNN','Decision Tree','Gradient Boost','Logistic Regression','xgboost'],\n    'optScore': [svc_cv.best_score_,knn_cv.best_score_,tree_cv.best_score_,gbk_cv.best_score_,logreg_cv.best_score_,xgb_cv.best_score_]})\noptmodels.sort_values(by='optScore', ascending=False)",
      "execution_count": 81,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "65fb2739-05e5-4d1b-a756-9b4decc865a1",
        "_uuid": "376c10802e125a57ebc4aa82fb1468c075f3c9a2"
      },
      "cell_type": "markdown",
      "source": "## Comparing the results of the tuned models (accuracy)"
    },
    {
      "metadata": {
        "_cell_guid": "622c2708-df4e-4c53-b14d-ab9c5020f926",
        "_uuid": "9592eaaa4a63b22f38f7bb9dcaba218af94483cc",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "optmodels = pd.DataFrame({\n    'optModel': ['Linear Regression','KNearestNieghbours','Decision Tree','Gradient Boost','Logistic Regression','xgboost'],\n    'optScore': [acc_svc_cv,acc_knn_cv,acc_tree_cv,acc_gbk_cv,acc_logreg_cv,acc_xgb_cv]})\noptmodels.sort_values(by='optScore', ascending=False)",
      "execution_count": 82,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "a6db44f8-1c7e-487a-944a-65d2295e9653",
        "_uuid": "29b6c32d918bf9443b011934423c4264d3d67592"
      },
      "cell_type": "markdown",
      "source": "# Optimising the Model\n\nAdding parameters to the basic models generally improved the performance on the training data. These gain on the training data did not always translate to the same increase in performance on the test data, due to over fitting. "
    },
    {
      "metadata": {
        "_cell_guid": "f337ac46-3fa0-47d3-b9c5-ca96d96f8278",
        "_uuid": "51abbe997d1cc5221c5752e804f01e7b29ee50af"
      },
      "cell_type": "markdown",
      "source": "# Predictions based on tuned model"
    },
    {
      "metadata": {
        "_cell_guid": "381fa290-fb7a-4950-8d60-a94d10b4a3e2",
        "_uuid": "e91b37e071b2ea2ec02bc8efa727ab5831038ca1",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Select columns\nfrom sklearn.tree import DecisionTreeClassifier\ntest = df_test[REVISED_NUMERIC_COLUMNS].fillna(-1000)\n# select classifier\n#tree = DecisionTreeClassifier(random_state=0,max_depth=5,max_features=7,min_samples_leaf=2,criterion=\"entropy\") #85,87\n#knn = KNeighborsClassifier(algorithm='kd_tree',leaf_size=20,n_neighbors=5)\n#logreg = LogisticRegression(solver='newton-cg')\n#xgboost=XGBClassifier(n_estimators= 300, max_depth= 10, learning_rate= 0.01)\n#Tuned Decision Tree Parameters: {'random_state': 0, 'min_samples_leaf': 2, 'max_features': 7, 'max_depth': 5, 'criterion': 'entropy'} #85,87\n#Tuned Decision Tree Parameters: {'random_state': 1, 'min_samples_leaf': 9, 'max_features': 8, 'max_depth': 6, 'criterion': 'entropy'}#84,88\n#tree = DecisionTreeClassifier(class_weight=None, criterion='entropy', max_depth=4,max_features=7, max_leaf_nodes=None, min_impurity_decrease=0.0,min_impurity_split=None, min_samples_leaf=9,min_samples_split=2, min_weight_fraction_leaf=0.0,presort=False, random_state=8, splitter='best')\ntree = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=4,max_features=7, max_leaf_nodes=None, min_impurity_decrease=0.0,min_impurity_split=None, min_samples_leaf=9,min_samples_split=2, min_weight_fraction_leaf=0.0,presort=False, random_state=9, splitter='best')\n#gbk=GradientBoostingClassifier(min_samples_leaf=1,max_features=4,max_depth=5)\nlogreg=LogisticRegression(solver='liblinear',C= 100)\n# train model\ntree.fit(data_to_train, prediction)\n# make predictions\nSubmission['Survived']=tree.predict(test)\nprint(Submission.head(5))",
      "execution_count": 84,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "b661a786-5d82-43ec-8c49-9e1c652a577b",
        "_uuid": "d6a49f130fa3e2b5bab23ebf4282a1c3b74368f7"
      },
      "cell_type": "markdown",
      "source": "# Make Hyper Tuned model submission"
    },
    {
      "metadata": {
        "_cell_guid": "c21a4928-1d97-4dd8-afeb-de89c97962c6",
        "_uuid": "bbd022d8385223bd6a973d7f8a179ad2444355ae",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# write data frame to csv file\n# Submission.set_index('PassengerId', inplace=True)\nSubmission.to_csv('treesubmission04.csv',sep=',')\nprint('File created')",
      "execution_count": 100,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "b95554ff-0b03-4ddb-a252-75a59a9814ad",
        "_uuid": "cf26d67b6c7ec409679aafba18a05a9133871502"
      },
      "cell_type": "markdown",
      "source": "# Stage 5 : Hyper tuning with confusion matrix\n\nI used a grid search cross validation in the previous stages to estimate the best results, we can use a confusion matrix to find out how well this model works by penalizing incorrect predictions."
    },
    {
      "metadata": {
        "_cell_guid": "9a65233e-e4ed-45f6-a184-2b09050e90d1",
        "_uuid": "bc07c290335e1e47761a5ec3d935a3f21c1f4fc7",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# knn Hyper Tunning with confusion Matrix\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold,StratifiedKFold, learning_curve #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import confusion_matrix #for confusion matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics #accuracy measure\nfrom sklearn.neighbors import KNeighborsClassifier\n\nREVISED_NUMERIC_COLUMNS=['Pclass','Age','SibSp','Parch','Family_Survival','Alone','Sex_female','Sex_male','Title_Master', 'Title_Miss','Title_Mr', 'Title_Mrs', 'Title_Millitary','Embarked'] #84\n\n# create test and training data\ndata_to_train = df_train[REVISED_NUMERIC_COLUMNS].fillna(-1000)\nX_test2= df_test[REVISED_NUMERIC_COLUMNS].fillna(-1000)\nprediction  = df_train[\"Survived\"]\nX_train, X_test, y_train, y_test = train_test_split(data_to_train, prediction, test_size = 0.3,random_state=21, stratify=prediction)\nprint('Data Split')\n\nhyperparams = {'algorithm': ['auto'], 'weights': ['uniform', 'distance'] ,'leaf_size': list(range(1,50,5)), \n               'n_neighbors':[6,7,8,9,10,11,12,14,16,18,20,22]}\ngd=GridSearchCV(estimator = KNeighborsClassifier(), param_grid = hyperparams, verbose=True, cv=10, scoring = \"roc_auc\")\ngd.fit(X_train, y_train)\n\ngd.best_estimator_.fit(X_train,y_train)\ny_pred=gd.best_estimator_.predict(X_test)\nSubmission['Survived']=gd.best_estimator_.predict(X_test2)\n\n# Print the results\nprint('Best Score')\nprint(gd.best_score_)\nprint('Best Estimator')\nprint(gd.best_estimator_)\nacc_gd_cv = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint('Accuracy')\nprint(acc_gd_cv)\n\n# Generate the confusion matrix and classification report\nprint('Confusion Matrrix')\nprint(confusion_matrix(y_test, y_pred))\nprint('Classification_report')\nprint(classification_report(y_test, y_pred))\n#Submission.set_index('PassengerId', inplace=True)\nprint('Sample Prediction')\nprint(Submission.head(10))\n#Submission.to_csv('knngridsearch03.csv',sep=',')\nprint('KNN prediction created')",
      "execution_count": 86,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "32d6b403-78f0-4a09-923b-0287b5bdda86",
        "_uuid": "908b81f5bccc4e6933b4dffa6616778ff2c61d1c",
        "trusted": false,
        "collapsed": true
      },
      "cell_type": "code",
      "source": "# Decision Tree Hyper Tunning with confusion Matrix\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import confusion_matrix #for confusion matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics #accuracy measure\nfrom sklearn.tree import DecisionTreeClassifier\n\nREVISED_NUMERIC_COLUMNS=['Pclass','AgeBand','FareBand','SibSp','Parch','Family_Survival','Alone','Sex_female','Sex_male','Title_Master', 'Title_Miss','Title_Mr', 'Title_Mrs', 'Title_Millitary','Embarked'] #84\n\n# create test and training data\ndata_to_train = df_train[REVISED_NUMERIC_COLUMNS].fillna(-1000)\nX_test2= df_test[REVISED_NUMERIC_COLUMNS].fillna(-1000)\nprediction  = df_train[\"Survived\"]\nX_train, X_test, y_train, y_test = train_test_split(data_to_train, prediction, test_size = 0.3,random_state=21, stratify=prediction)\nprint('Data Split')\n\nhyperparams = {\"random_state\" :  np.arange(0, 10),\n              \"max_depth\": np.arange(1, 10),\n              \"max_features\": np.arange(1, 10),\n              \"min_samples_leaf\": np.arange(1, 10),\n              \"criterion\": [\"gini\",\"entropy\"]}\n\ngd=GridSearchCV(estimator = DecisionTreeClassifier(), param_grid = hyperparams, verbose=True, cv=10, scoring = \"roc_auc\")\ngd.fit(X_train, y_train)\n\ngd.best_estimator_.fit(X_train,y_train)\ny_pred=gd.best_estimator_.predict(X_test)\nSubmission['Survived']=gd.best_estimator_.predict(X_test2)\n\n# Print the results\nprint('Best Score')\nprint(gd.best_score_)\nprint('Best Estimator')\nprint(gd.best_estimator_)\nacc_gd_cv = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint('Accuracy')\nprint(acc_gd_cv)\n\n# Generate the confusion matrix and classification report\nprint('Confusion Matrrix')\nprint(confusion_matrix(y_test, y_pred))\nprint('Classification_report')\nprint(classification_report(y_test, y_pred))\n#Submission.set_index('PassengerId', inplace=True)\n# print head\nprint(Submission.head(10))\nSubmission.to_csv('Treegridsearch03.csv',sep=',')\nprint('Decision Tree prediction created')",
      "execution_count": 87,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "a9579ea384ad6594e940f4dc9ced489f0450916e"
      },
      "cell_type": "markdown",
      "source": "# Plotting Learning Curves"
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "3401f708c32d012de64463a1b3cbd9ea9af14ef3",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from sklearn.model_selection import GridSearchCV,cross_val_score,cross_val_predict,cross_validate,KFold,StratifiedKFold, learning_curve\n\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\n# Cross validate model with Kfold stratified cross val\nkfold = StratifiedKFold(n_splits=10)\n\ng = plot_learning_curve(svc_cv.best_estimator_,\"linear regression learning curves\",X_train,y_train,cv=kfold)\ng = plot_learning_curve(logreg_cv.best_estimator_,\"logistic regression learning curves\",X_train,y_train,cv=kfold)\ng = plot_learning_curve(knn_cv.best_estimator_,\"knn learning curves\",X_train,y_train,cv=kfold)\ng = plot_learning_curve(tree_cv.best_estimator_,\"decision tree learning curves\",X_train,y_train,cv=kfold)\ng = plot_learning_curve(randomforest_cv.best_estimator_,\"random forest learning curves\",X_train,y_train,cv=kfold)\ng = plot_learning_curve(xgb_cv.best_estimator_,\"xg boost learning curves\",X_train,y_train,cv=kfold)",
      "execution_count": 91,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "03700e7b7ac06c25b48824b181285b6ab1f2a6d2"
      },
      "cell_type": "markdown",
      "source": "## Plot Area under ROC"
    },
    {
      "metadata": {
        "trusted": false,
        "_uuid": "05d20e6d573c5ba96add61e1efe47b2088867163",
        "collapsed": true
      },
      "cell_type": "code",
      "source": "from sklearn import ensemble, linear_model,neighbors, svm, tree\nfrom sklearn.metrics import mean_squared_error,confusion_matrix, precision_score, recall_score, auc,roc_curve\n\n# List of Machine Learning Algorithm (MLA)\nMLA = [\n    #Ensemble Methods\n    ensemble.ExtraTreesClassifier(),\n    ensemble.GradientBoostingClassifier(),\n    ensemble.RandomForestClassifier(),\n    \n    #GLM\n    linear_model.LogisticRegressionCV(),\n    \n    #Nearest Neighbor\n    neighbors.KNeighborsClassifier(),\n    \n    #SVM\n    svm.SVC(probability=True),\n    \n    #Trees    \n    tree.DecisionTreeClassifier(),\n   #tree.ExtraTreeClassifier(), \n    ]\n\nindex = 1\nfor alg in MLA:\n    predicted = alg.fit(X_train, y_train).predict(X_test)\n    fp, tp, th = roc_curve(y_test, predicted)\n    roc_auc_mla = auc(fp, tp)\n    MLA_name = alg.__class__.__name__\n    plt.plot(fp, tp, lw=2, alpha=0.3, label='ROC %s (AUC = %0.2f)'  % (MLA_name, roc_auc_mla))\n    index+=1\n\nplt.title('ROC Curve comparison')\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([0,1])\nplt.ylim([0,1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')    \nplt.show()",
      "execution_count": 98,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "664eb1bd-bd71-41d8-b89e-66625c19adf7",
        "_uuid": "28a3aebb2b2b075a0fa0ed4dd6c2b987c72f3794"
      },
      "cell_type": "markdown",
      "source": "# Stage 6 : Basic Ensemble Modelling\n\nIn the last couple of stages I tried a few different models with  differnet parameters to try and find the one that produced the best results. Another approach would be to use an Ensemble model, that generates results from a selection of the best performing models and then feeds the results into a another model in a second layer.  "
    },
    {
      "metadata": {
        "_cell_guid": "6b61eea3-4223-44e6-8e44-3651cf2c9c75",
        "_uuid": "397fd8c5fa1b2bed8a8cc39ff18740caac01d0e8",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Split Data\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score, cross_val_predict\n\nNUMERIC_COLUMNS=['Pclass','Age','SibSp','Parch','Sex_female','Sex_male','Title_Master', 'Title_Miss','Title_Mr', 'Title_Mrs', 'Title_Services','FareBand','Embarked','Alone','Family Size']\nREVISED_NUMERIC_COLUMNS=['Pclass','Age','SibSp','Parch','Family_Survival','Alone','Sex_female','Sex_male','Title_Master', 'Title_Miss','Title_Mr', 'Title_Mrs', 'Title_Millitary','Embarked'] #84\n\n# create test and training data\ndata_to_train = df_train[REVISED_NUMERIC_COLUMNS].fillna(-1000)\nprediction = df_train[\"Survived\"]\ndf_test=df_test[REVISED_NUMERIC_COLUMNS].fillna(-1000)\nX_train, X_val, y_train, y_val = train_test_split(data_to_train, prediction, test_size = 0.3,random_state=21, stratify=y)\nprint('Data Split')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "4f86d505-5400-4bfb-ad8a-d63474ea773b",
        "_uuid": "f1c8ddcc4661a1f70fa07316e5e03ffc4e240ab2"
      },
      "cell_type": "markdown",
      "source": "## Train first layer "
    },
    {
      "metadata": {
        "_cell_guid": "cbc46591-6855-4991-9287-9981d25eb0aa",
        "_uuid": "1ce24cf6fcb4d36fa33b731178b4adc7e38d4db3",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "#logreg = LogisticRegression()\nlogreg = LogisticRegression(C=1, solver='newton-cg')\nlogreg.fit(X_train, y_train)\ny_pred_train_logreg = cross_val_predict(logreg,X_val, y_val)\ny_pred_test_logreg = logreg.predict(X_test)\nprint('logreg first layer predicted')\n\n#tree = DecisionTreeClassifier()\ntree = DecisionTreeClassifier(random_state=8,min_samples_leaf=6, max_features= 7, max_depth= 4, criterion='gini')\ntree.fit(X_train, y_train)\ny_pred_train_tree = cross_val_predict(tree,X_val,y_val)\ny_pred_test_tree = tree.predict(X_test)\nprint('decision tree first layer predicted')\n\n# randomforest = RandomForestClassifier()\nrandomforest = RandomForestClassifier(random_state=8, n_estimators=15, min_samples_leaf= 4, max_features= 6, max_depth=4,criterion='gini')\nrandomforest.fit(X_train, y_train)\ny_pred_train_randomforest = cross_val_predict(randomforest, X_val, y_val)\ny_pred_test_randomforest = randomforest.predict(X_test)\nprint('random forest first layer predicted')\n\n# gbk = GradientBoostingClassifier()\ngbk = GradientBoostingClassifier(min_samples_leaf=3, max_features= 3, max_depth= 3)\ngbk.fit(X_train, y_train)\ny_pred_train_gbk = cross_val_predict(randomforest, X_val, y_val)\ny_pred_test_gbk = gbk.predict(X_test)\nprint('gbk first layer predicted')\n\n# instanciate classifier\n#clf = SVC()\nclf = SVC(C=3, degree=1, kernel='linear', max_iter=1, shrinking=0)\nclf.fit(X_train, y_train)\ny_pred_train_clf = cross_val_predict(clf, x_val, y_val)\ny_pred_test_clf = clf.predict(X_test)\nprint('clf first layer predicted')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "c93c7250-3728-4af4-99e4-0d41a92bdbf7",
        "_uuid": "df4e474272a912e8e4030e8dc32bc936387125db"
      },
      "cell_type": "markdown",
      "source": "### Checkshape of prediction data"
    },
    {
      "metadata": {
        "_cell_guid": "f414ecd8-2597-4644-92ec-6ce55460c4e6",
        "_uuid": "0e98a4d0273e52f2a5a11b26c4040655ebd0dd8f",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "print(y_pred_train_logreg.shape)\nprint(y_pred_train_gbk.shape)\nprint(y_pred_train_tree.shape)\nprint(y_pred_train_randomforest.shape)\nprint(y_pred_train_clf.shape)\nprint(y_val.shape)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "e00799d5-44f3-4748-bfa4-66574a31e191",
        "_uuid": "ccab0e2d5cafdad98f95a592539313644073bc42",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Create Ensemble Model baseline (no hypertuning yet!)\nfrom sklearn.neighbors import KNeighborsClassifier\nsecond_layer_train = pd.DataFrame( {'Logistic Regression': y_pred_train_logreg.ravel(),\n                                    'Gradient Boosting': y_pred_train_gbk.ravel(),\n                                    'Decision Tree': y_pred_train_tree.ravel(),\n                                    'Random Forest': y_pred_train_randomforest.ravel(),\n                                    'clf': y_pred_train_clf.ravel()\n                                    } )\nsecond_layer_train.head()\n\nX_train_second = np.concatenate(( y_pred_train_logreg.reshape(-1, 1), y_pred_train_gbk.reshape(-1, 1), \n                                  y_pred_train_tree.reshape(-1, 1), y_pred_train_randomforest.reshape(-1, 1),y_pred_train_clf.reshape(-1, 1)),axis=1)\nX_test_second = np.concatenate(( y_pred_test_logreg.reshape(-1, 1), y_pred_test_gbk.reshape(-1, 1), \n                                 y_pred_test_tree.reshape(-1, 1), y_pred_test_randomforest.reshape(-1, 1),y_pred_train_clf.reshape(-1, 1)),axis=1)\n\nknn = KNeighborsClassifier().fit(X_train_second,y_val)\n\nSubmission['Survived'] = knn.predict(X_test_second)\nprint(Submission.head())\nprint('Basic Ensemble model prediction complete')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "d9c1384b-9003-41cd-9241-54de84055c79",
        "_uuid": "064d274bb8d0791acf17494a91ea1252273706e9",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# write data frame to csv file\nSubmission.set_index('PassengerId', inplace=True)\nSubmission.to_csv('basicensemblesubmission04.csv',sep=',')\nprint('Basic Ensemble File created')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "55a0929c-d6c6-4c53-84bf-02aef849ed45",
        "_uuid": "c4b73f33042736506e78e876c327e0e3b00610a7"
      },
      "cell_type": "markdown",
      "source": "# Stage 7 : Hyper Tuned Ensemble Modelling"
    },
    {
      "metadata": {
        "_cell_guid": "dfcb96e2-ce47-47df-8199-0a8d5c85c1d8",
        "_uuid": "c8a2b6adba0cbc0c9bc11c99799523eb2c739e4e",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# Create Ensemble Model baseline (tuned model!)\nsecond_layer_train = pd.DataFrame( {'Logistic Regression': y_pred_train_logreg.ravel(),\n                                    'Gradient Boosting': y_pred_train_gbk.ravel(),\n                                    'Decision Tree': y_pred_train_tree.ravel(),\n                                    'Random Forest': y_pred_train_randomforest.ravel()\n                                    } )\n\nX_train_second = np.concatenate(( y_pred_train_logreg.reshape(-1, 1), y_pred_train_gbk.reshape(-1, 1), \n                                  y_pred_train_tree.reshape(-1, 1), y_pred_train_randomforest.reshape(-1, 1)),\n                                  axis=1)\nX_test_second = np.concatenate(( y_pred_test_logreg.reshape(-1, 1), y_pred_test_gbk.reshape(-1, 1), \n                                 y_pred_test_tree.reshape(-1, 1), y_pred_test_randomforest.reshape(-1, 1)),\n                                 axis=1)\n#xgb = XGBClassifier(n_estimators= 800,max_depth= 4,min_child_weight= 2,gamma=0.9,subsample=0.8,colsample_bytree=0.8,objective= 'binary:logistic',nthread= -1,scale_pos_weight=1).fit(X_train_second, y_val)\ntree = DecisionTreeClassifier(random_state=8,min_samples_leaf=6, max_depth= 4, criterion='gini').fit(X_train_second,y_val)\n\nSubmission['Survived'] = tree.predict(X_test_second)\nprint(Submission.head())\nprint('Tuned Ensemble model prediction complete')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "d9f64360-c860-4974-a1f7-18e1d754180c",
        "_uuid": "d94bdb874ae16aaff9a0f2111754b57a79eb8afd",
        "collapsed": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "# write data frame to csv file\n#Submission.set_index('PassengerId', inplace=True)\n#Submission.to_csv('tunedensemblesubmission04.csv',sep=',')\nprint('tuned Ensemble File created')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "c0162b2e-f110-4d6a-aeea-6493f5e5a907",
        "_uuid": "ddbacdcb188b189db5cb581c7cd748bb8bf144b5"
      },
      "cell_type": "markdown",
      "source": "# Summary\n\nIn this project we have explored the Titanic Data Set, we have identified missing data and filled then as best we could, we have converted categorical data to columns of numeric features that we can use in machine learning and we have engineered new features based on the data we had. We improved our score from base line of 0.57894 to  a score of 0.78.\n\nGoing from a score of 0.57 to 0.77 was the relatively easy part, taking it from 7.8 to 0.8 is a whole different ball game. Its really temping to overwork the data trying to find new features that might improve the score but in really what you gain in new features you loose in the noise you've introduce, its also tempting to keep tweak the parameters of your model to get the best possible score on the test data, but gain what you gain in performance on the training data you loose in overfitting. A better approach is to stick to the features that have the strongest relationships and ensure that any data that you are estimating or engineering is as accurate as you can possibly make it. Using cross validation to hyper tune the model while minimising any over fitting of the data.\n\nWhen I initially created the project I kept the test and training data completely separate but am I am rapidly coming to the conclusion that combining the two datasets,  is possibly a better approach for estimating missing data based on averages across the entire dataset. \n\nI  looked at a range of different models and compared the accuracy of each model on the training data before deciding which model to use for the third submission. I then hyper tuned  a hanful of the best performing to ensure that I submitted the best performing hyper tuned model. \n\nHaving hypertuned a single model the next step in my process was to attempt combining several models in an ensemble. I managed to achieve a result of .803 which was OK but not as good as the best hypertuned models that i'd produced.\n\nI havn't come any where near winning this contest yet, but I survived my first Kaggle contest and got a score of over .8 which has my goal. The main thing is that I had fun and learnt a lot along the way by trying different techniques and looking at what other people were doing.\n\nI've also created a kernal that uses the same data with deep learning, you can find this at https://www.kaggle.com/davidcoxon/deeply-titanic"
    },
    {
      "metadata": {
        "_cell_guid": "ee85a578-c963-42d8-ac8f-0097737527ab",
        "_uuid": "2cbd72ea1657a90ba05e837969cac1f4d8530bb0"
      },
      "cell_type": "markdown",
      "source": "# Credit where credits due\n\nThis competition is predominantly a training exercise and as such I have tried to looks at different approaches and try different techniques to see hw they work.  I have looked at some of the existing entries and adopted some of the tequiques that i have found interesting. So firstly a huge thanks to everyone that look the time to document their code and explain step by step what they did and why.\n\nTo naming names, some of the notebooks that i found most useful and think deserve special mensions are:\n\n### Aldemuro M.A.Haris\nhttps://www.kaggle.com/aldemuro/comparing-ml-algorithms-train-accuracy-90\nInteresting model comparison and ROC graphs\n\n\n### Anisotropic\nhttps://www.kaggle.com/arthurtok/introduction-to-ensembling-stacking-in-python/notebook\n\nIntroduction to Ensembling/Stacking in Python is a very useful project on many levels, in I particular I liked how elegantly this code was written.\n\n### Bisaria\nhttps://www.kaggle.com/bisaria/titanic-lasso-ridge-implementation/code\n\nWhile this notebook is based on R and I am working in Python, I found some of the visualizations interesting, specifically the port of embarkation and number of siblings and the mosaic. I also liked the idea of the lone traveller feature and the allocation of the cabin data, based on family.\n\n### CalebCastleberry\nhttps://www.kaggle.com/ccastleberry/titanic-cabin-features\n\nThis notebook explains the importance of the deck feature and proves you can score 70% on the deck feature alone.\n\n### Henrique Mello \nhttps://www.kaggle.com/hrmello/introduction-to-data-exploration-using-seaborn/notebook\n\nThis has some great visualisations of the data and helped me understand the importance of using title in predicting ages when filling in the missing data. \n\n### Konstantin\nhttps://www.kaggle.com/konstantinmasich/titanic-0-82-0-83\n\n### LD Freeman\nhttps://www.kaggle.com/ldfreeman3/a-data-science-framework-to-achieve-99-accuracy\n\nThis not only achieves a fantastic score but is a great tutorial on data science techniques\n\n### Nadin Tamer\nhttps://www.kaggle.com/nadintamer/titanic-survival-predictions-beginner/notebook\n\nI found this another really useful kernel. It is very much a step by step approach, with a particularly good section on different types of model and how they perform for this project.\n\n### Omar El Gabry\nhttps://www.kaggle.com/omarelgabry/a-journey-through-titanic?scriptVersionId=447802/notebook\n\nThis kernal has an interesting section on estimating the missing ages and calculating pearson co-efficients for the features.\n\n### Oscar Takeshita\nhttps://www.kaggle.com/pliptor/divide-and-conquer-0-82296/code\n\nThis kernal was very useful in trying to get over the 0.8 ceiling, its based on R rather than python so i haven't used any of the code, but it helped me focus on the key fearures and to see the benefits of uing the combined training and test dataset for statistics and calculations rather keeping the two at arms length. \n\n### Sina\nhttps://www.kaggle.com/sinakhorami/titanic-best-working-classifier?scriptVersionId=566580\n\nA lot of high scoring kernals reference this notebook, especially the feature engineering discussed in it.\n\n### S.Xu\nhttps://www.kaggle.com/shunjiangxu/blood-is-thicker-than-water-friendship-forever\n\nThis kernal is based on an original kernal by Sina, and it uses the last name and ticket details to find families and firends it then looks at the survival of the group as a whole.\n\n### Yassine Ghouzam\nhttps://www.kaggle.com/yassineghouzam/titanic-top-4-with-ensemble-modeling\n\nThis kernal has an interesting section on learning curves."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.4",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}